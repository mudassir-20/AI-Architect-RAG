# ğŸ—ï¸ The AI Architect-RAG

# Enterprise RAG Agent with Self-Correction

The Architect is an enterprise-grade Retrieval-Augmented Generation (RAG) system designed to answer questions from large documents reliably, safely, and transparently.

Unlike basic RAG demos, this project implements multi-agent self-correction, source-aware answers, and automatic web fallback, closely mirroring how production AI systems are built in real companies.

## ğŸš€ Key Features

**ğŸ“„ Document-based Question Answering**

Answers questions strictly from a provided PDF (NVIDIA Annual Report) using semantic search.

**ğŸ§  Multi-Query Retrieval**

Automatically rewrites user queries into multiple semantic variants to improve recall.

**ğŸ” Multi-Agent Self-Correction Pipeline**

* Answer Agent â€“ generates an initial response
* Critic Agent â€“ evaluates correctness, grounding, and clarity
* Refiner Agent â€“ improves the answer based on critic feedback

**ğŸ›‘ Hallucination Control**

* Verifies whether answers are supported by document context
* Responds with â€œI donâ€™t know based on the documentâ€ when appropriate

**ğŸŒ Web Search Fallback**

* Automatically detects external questions
* Uses web search (Tavily API) when the document does not contain the answer
* Clearly separates document-based vs web-based answers

**ğŸ§¾ Source-Aware Responses**

* Adds page-level citations for document answers
* Never attaches document sources to web-based answers

**ğŸ’¬ Conversational Memory**

Maintains short-term chat history for follow-up questions

**âš™ï¸ FastAPI Deployment**

* Exposes the agent via a clean /ask API endpoint
* Ready for frontend integration or production deployment

## ğŸ§  System Architecture

User Query
 â¡ï¸
Query Rewriter (Multi-query retrieval)
   â¡ï¸
Vector Database (ChromaDB + PDF embeddings)
   â¡ï¸
Answer Agent (LLM)
   â¡ï¸
Critic Agent (quality & grounding check)
   â¡ï¸
Refiner Agent (self-correction)
   â¡ï¸
Verifier (hallucination guard)

   â¬‡ï¸
   
Decision Router

   â”œâ”€ Document Answer + Page Sources
   
   â””â”€ Web Search Fallback + Web Answer
   
 â¬‡ï¸
 
Final Response + Memory

## ğŸ› ï¸ Tech Stack

* LLM: Groq (LLaMA 3.x)

* Embeddings: Sentence-Transformers (MiniLM)
* Vector Database: ChromaDB

* Web Search: Tavily API

* Backend: FastAPI

* Language: Python

* Deployment Environment: GitHub Codespaces



