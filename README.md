# ğŸ—ï¸ The AI Architect-RAG

# Enterprise RAG Agent with Self-Correction

The Architect is an enterprise-grade Retrieval-Augmented Generation (RAG) system designed to answer questions from large documents reliably, safely, and transparently.

Unlike basic RAG demos, this project implements multi-agent self-correction, source-aware answers, and automatic web fallback, closely mirroring how production AI systems are built in real companies.

## ğŸš€ Key Features

**ğŸ“„ Document-based Question Answering**

Answers questions strictly from a provided PDF (NVIDIA Annual Report) using semantic search.

**ğŸ§  Multi-Query Retrieval**

Automatically rewrites user queries into multiple semantic variants to improve recall.

**ğŸ” Multi-Agent Self-Correction Pipeline**

* Answer Agent â€“ generates an initial response
* Critic Agent â€“ evaluates correctness, grounding, and clarity
* Refiner Agent â€“ improves the answer based on critic feedback

**ğŸ›‘ Hallucination Control**

* Verifies whether answers are supported by document context
* Responds with â€œI donâ€™t know based on the documentâ€ when appropriate

**ğŸŒ Web Search Fallback**

* Automatically detects external questions
* Uses web search (Tavily API) when the document does not contain the answer
* Clearly separates document-based vs web-based answers

**ğŸ§¾ Source-Aware Responses**

* Adds page-level citations for document answers
* Never attaches document sources to web-based answers

**ğŸ’¬ Conversational Memory**

Maintains short-term chat history for follow-up questions

**âš™ï¸ FastAPI Deployment**

* Exposes the agent via a clean /ask API endpoint
* Ready for frontend integration or production deployment

## ğŸ§  System Architecture

User Query
 â¡ï¸
Query Rewriter (Multi-query retrieval)
   â¡ï¸
Vector Database (ChromaDB + PDF embeddings)
   â¡ï¸
Answer Agent (LLM)
   â¡ï¸
Critic Agent (quality & grounding check)
   â¡ï¸
Refiner Agent (self-correction)
   â¡ï¸
Verifier (hallucination guard)

   â¬‡ï¸
   
Decision Router

   â”œâ”€ Document Answer + Page Sources
   
   â””â”€ Web Search Fallback + Web Answer
   
 â¬‡ï¸
 
Final Response + Memory

## ğŸ› ï¸ Tech Stack

* LLM: Groq (LLaMA 3.x)

* Embeddings: Sentence-Transformers (MiniLM)
* Vector Database: ChromaDB

* Web Search: Tavily API

* Backend: FastAPI

* Language: Python

* Deployment Environment: GitHub Codespaces

## ğŸ“‚ Project Structure

â”œâ”€â”€ api.py          # FastAPI application

â”œâ”€â”€ query.py        # Core RAG + agent logic

â”œâ”€â”€ ingest.py       # PDF ingestion & embedding

â”œâ”€â”€ store.py        # Vector store utilities

â”œâ”€â”€ data.pdf        # Source document (NVIDIA Annual Report)

â”œâ”€â”€ README.md       # Project documentation

â””â”€â”€ .gitignore

## ğŸ¯ Why This Project Matters

Most RAG examples stop at retrieval + generation.

This project goes further by adding:

* Self-correction

* Verification

* Provenance control

* Web fallback decision logic

### ğŸ” Example Queries

What is NVIDIA Omniverse?

Explain it in simple terms

Who is the CEO of NVIDIA in 2025?

The system will automatically decide whether to answer from the document or use web search.

### ğŸ‘¤ Author

Mudassir Ansari

Computer Science (AI & ML)

Aspiring AI Engineer




